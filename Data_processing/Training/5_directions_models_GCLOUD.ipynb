{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_photos(directory):\n",
    "    images = []\n",
    "    labels_speed = []\n",
    "    labels_dir = []\n",
    "    dir_list = listdir(directory)\n",
    "    random.shuffle(dir_list)\n",
    "    for name in dir_list:\n",
    "        filename = directory + '/' + name\n",
    "        # load an image from file\n",
    "        image = load_img(filename, target_size=(96, 160))\n",
    "        # convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "        # get image id + labels\n",
    "        value_dir = float(name.split('_')[1])\n",
    "        value_speed = float(name.split('_')[0])\n",
    "        labels_dir.append(value_dir)\n",
    "        labels_speed.append(value_speed)\n",
    "        images.append(image)\n",
    "    return images, labels_speed, labels_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = \"BigOne_Race_5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Images and labels for training: 7953\n"
     ]
    }
   ],
   "source": [
    "# load images from both train and test groups\n",
    "directory = dataset+'/Train'\n",
    "images, labels_speed, labels_dir = load_photos(directory)\n",
    "nb_images = len(images)\n",
    "print('Loaded Images and labels for training: %d' % nb_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalise images\n",
    "images = np.array(images)\n",
    "images /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert datas to dummyvalues\n",
    "labels_speed = np.array(pd.get_dummies(labels_speed))\n",
    "labels_dir = np.array(pd.get_dummies(labels_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0], dtype=uint8),\n",
       " array([0, 0, 0, 1, 0], dtype=uint8),\n",
       " <matplotlib.image.AxesImage at 0x7fcbce465be0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADrCAYAAAB5JG1xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcbce5bf8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_speed[42], labels_dir[42], plt.imshow(images[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "import keras.backend as K\n",
    "import keras.optimizers as Optimizers\n",
    "import keras.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "#model = load_model(\"model_bigrace_original_5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img_in (InputLayer)             (None, 96, 160, 3)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 46, 78, 2)    150         img_in[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 46, 78, 2)    8           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 46, 78, 2)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 21, 37, 4)    200         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 21, 37, 4)    16          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 21, 37, 4)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 9, 17, 8)     800         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 9, 17, 8)     32          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 9, 17, 8)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flattened (Flatten)             (None, 1224)         0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1224)         0           flattened[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          122400      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 50)           5000        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 50)           200         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 50)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 50)           0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           500         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           500         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 10)           40          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 10)           40          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 10)           0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 10)           0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 10)           0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 10)           0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            22          dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 5)            55          dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 130,363\n",
      "Trainable params: 129,995\n",
      "Non-trainable params: 368\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Model from PatateV2\n",
    "'''\n",
    "\n",
    "K.clear_session()\n",
    "############################################################# \n",
    "\n",
    "img_in = Input(shape=(96, 160, 3), name='img_in')\n",
    "x = img_in\n",
    "\n",
    "x = Convolution2D(2, (5,5), strides=(2,2), use_bias=False)(x)       \n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Convolution2D(4, (5,5), strides=(2,2), use_bias=False)(x)       \n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Convolution2D(8, (5,5), strides=(2,2), use_bias=False)(x)       \n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "x = Flatten(name='flattened')(x)\n",
    "\n",
    "x = Dropout(.5)(x)\n",
    "x = Dense(100, use_bias=False)(x) \n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(.4)(x)\n",
    "x = Dense(50, use_bias=False)(x)  \n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(.3)(x)\n",
    "\n",
    "# Multitask Output\n",
    "out_speed = Dense(10, use_bias=False)(x)  \n",
    "out_speed = BatchNormalization()(out_speed)\n",
    "out_speed = Activation(\"relu\")(out_speed)\n",
    "out_speed = Dropout(.2)(out_speed)\n",
    "out_speed = Dense(2, activation='softmax')(out_speed)\n",
    "\n",
    "out_dir = Dense(10, use_bias=False)(x)  \n",
    "out_dir = BatchNormalization()(out_dir)\n",
    "out_dir = Activation(\"relu\")(out_dir)\n",
    "out_dir = Dropout(.2)(out_dir)\n",
    "out_dir = Dense(5, activation='softmax')(out_dir)\n",
    "\n",
    "\n",
    "# Compile Model\n",
    "model = Model(inputs=[img_in], outputs=[out_speed, out_dir])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name=\"model_bigrace_original_5.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save best model if model improved\n",
    "best_checkpoint = keras.callbacks.ModelCheckpoint(model_name, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6362 samples, validate on 1591 samples\n",
      "Epoch 1/100\n",
      "6362/6362 [==============================] - 7s 1ms/step - loss: 2.2801 - dense_4_loss: 0.5621 - dense_6_loss: 1.7180 - dense_4_acc: 0.7287 - dense_6_acc: 0.2517 - val_loss: 2.5408 - val_dense_4_loss: 0.7207 - val_dense_6_loss: 1.8201 - val_dense_4_acc: 0.7511 - val_dense_6_acc: 0.2722\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.54076, saving model to model_bigrace_original_5.h5\n",
      "Epoch 2/100\n",
      "6362/6362 [==============================] - 4s 653us/step - loss: 2.0145 - dense_4_loss: 0.4897 - dense_6_loss: 1.5248 - dense_4_acc: 0.7680 - dense_6_acc: 0.3489 - val_loss: 8.1447 - val_dense_4_loss: 3.2441 - val_dense_6_loss: 4.9006 - val_dense_4_acc: 0.7511 - val_dense_6_acc: 0.1540\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/100\n",
      "6362/6362 [==============================] - 4s 657us/step - loss: 1.8634 - dense_4_loss: 0.4413 - dense_6_loss: 1.4222 - dense_4_acc: 0.7680 - dense_6_acc: 0.4057 - val_loss: 1.6612 - val_dense_4_loss: 0.4078 - val_dense_6_loss: 1.2534 - val_dense_4_acc: 0.7511 - val_dense_6_acc: 0.4972\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.54076 to 1.66121, saving model to model_bigrace_original_5.h5\n",
      "Epoch 4/100\n",
      "6362/6362 [==============================] - 4s 673us/step - loss: 1.7385 - dense_4_loss: 0.4015 - dense_6_loss: 1.3370 - dense_4_acc: 0.7793 - dense_6_acc: 0.4596 - val_loss: 1.5299 - val_dense_4_loss: 0.3671 - val_dense_6_loss: 1.1627 - val_dense_4_acc: 0.8366 - val_dense_6_acc: 0.5449\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.66121 to 1.52989, saving model to model_bigrace_original_5.h5\n",
      "Epoch 5/100\n",
      "6362/6362 [==============================] - 4s 674us/step - loss: 1.6215 - dense_4_loss: 0.3596 - dense_6_loss: 1.2619 - dense_4_acc: 0.8218 - dense_6_acc: 0.4943 - val_loss: 1.6970 - val_dense_4_loss: 0.4530 - val_dense_6_loss: 1.2440 - val_dense_4_acc: 0.7907 - val_dense_6_acc: 0.5009\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/100\n",
      "6362/6362 [==============================] - 4s 673us/step - loss: 1.5224 - dense_4_loss: 0.3290 - dense_6_loss: 1.1934 - dense_4_acc: 0.8414 - dense_6_acc: 0.5300 - val_loss: 1.3277 - val_dense_4_loss: 0.3071 - val_dense_6_loss: 1.0206 - val_dense_4_acc: 0.8705 - val_dense_6_acc: 0.6153\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.52989 to 1.32769, saving model to model_bigrace_original_5.h5\n",
      "Epoch 7/100\n",
      "6362/6362 [==============================] - 4s 662us/step - loss: 1.4679 - dense_4_loss: 0.3148 - dense_6_loss: 1.1531 - dense_4_acc: 0.8433 - dense_6_acc: 0.5530 - val_loss: 1.6704 - val_dense_4_loss: 0.4146 - val_dense_6_loss: 1.2558 - val_dense_4_acc: 0.7788 - val_dense_6_acc: 0.5097\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/100\n",
      "6362/6362 [==============================] - 4s 666us/step - loss: 1.3958 - dense_4_loss: 0.3117 - dense_6_loss: 1.0841 - dense_4_acc: 0.8441 - dense_6_acc: 0.5956 - val_loss: 1.4125 - val_dense_4_loss: 0.3507 - val_dense_6_loss: 1.0618 - val_dense_4_acc: 0.8347 - val_dense_6_acc: 0.5959\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/100\n",
      "6362/6362 [==============================] - 4s 672us/step - loss: 1.3281 - dense_4_loss: 0.2920 - dense_6_loss: 1.0361 - dense_4_acc: 0.8516 - dense_6_acc: 0.6154 - val_loss: 1.3555 - val_dense_4_loss: 0.3501 - val_dense_6_loss: 1.0054 - val_dense_4_acc: 0.8265 - val_dense_6_acc: 0.6524\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/100\n",
      "6362/6362 [==============================] - 4s 666us/step - loss: 1.2640 - dense_4_loss: 0.2836 - dense_6_loss: 0.9804 - dense_4_acc: 0.8557 - dense_6_acc: 0.6297 - val_loss: 1.2709 - val_dense_4_loss: 0.3322 - val_dense_6_loss: 0.9387 - val_dense_4_acc: 0.8378 - val_dense_6_acc: 0.6499\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.32769 to 1.27090, saving model to model_bigrace_original_5.h5\n",
      "Epoch 11/100\n",
      "6362/6362 [==============================] - 4s 685us/step - loss: 1.2270 - dense_4_loss: 0.2755 - dense_6_loss: 0.9515 - dense_4_acc: 0.8606 - dense_6_acc: 0.6412 - val_loss: 0.9783 - val_dense_4_loss: 0.2477 - val_dense_6_loss: 0.7306 - val_dense_4_acc: 0.9063 - val_dense_6_acc: 0.7209\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.27090 to 0.97832, saving model to model_bigrace_original_5.h5\n",
      "Epoch 12/100\n",
      "6362/6362 [==============================] - 4s 668us/step - loss: 1.1902 - dense_4_loss: 0.2696 - dense_6_loss: 0.9206 - dense_4_acc: 0.8640 - dense_6_acc: 0.6573 - val_loss: 1.0512 - val_dense_4_loss: 0.2795 - val_dense_6_loss: 0.7717 - val_dense_4_acc: 0.8806 - val_dense_6_acc: 0.7153\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/100\n",
      "6362/6362 [==============================] - 4s 662us/step - loss: 1.1348 - dense_4_loss: 0.2578 - dense_6_loss: 0.8771 - dense_4_acc: 0.8677 - dense_6_acc: 0.6680 - val_loss: 0.9990 - val_dense_4_loss: 0.2695 - val_dense_6_loss: 0.7295 - val_dense_4_acc: 0.8718 - val_dense_6_acc: 0.7190\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/100\n",
      "6362/6362 [==============================] - 4s 649us/step - loss: 1.0994 - dense_4_loss: 0.2576 - dense_6_loss: 0.8418 - dense_4_acc: 0.8788 - dense_6_acc: 0.6831 - val_loss: 0.9441 - val_dense_4_loss: 0.2573 - val_dense_6_loss: 0.6868 - val_dense_4_acc: 0.8938 - val_dense_6_acc: 0.7473\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.97832 to 0.94410, saving model to model_bigrace_original_5.h5\n",
      "Epoch 15/100\n",
      "6362/6362 [==============================] - 4s 687us/step - loss: 1.0697 - dense_4_loss: 0.2476 - dense_6_loss: 0.8220 - dense_4_acc: 0.8959 - dense_6_acc: 0.6886 - val_loss: 0.9480 - val_dense_4_loss: 0.2468 - val_dense_6_loss: 0.7012 - val_dense_4_acc: 0.8957 - val_dense_6_acc: 0.7511\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/100\n",
      "6362/6362 [==============================] - 4s 671us/step - loss: 1.0455 - dense_4_loss: 0.2491 - dense_6_loss: 0.7964 - dense_4_acc: 0.8989 - dense_6_acc: 0.7003 - val_loss: 1.0577 - val_dense_4_loss: 0.2878 - val_dense_6_loss: 0.7699 - val_dense_4_acc: 0.8674 - val_dense_6_acc: 0.7222\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/100\n",
      "6362/6362 [==============================] - 4s 657us/step - loss: 1.0014 - dense_4_loss: 0.2378 - dense_6_loss: 0.7636 - dense_4_acc: 0.9058 - dense_6_acc: 0.7149 - val_loss: 0.8293 - val_dense_4_loss: 0.2279 - val_dense_6_loss: 0.6013 - val_dense_4_acc: 0.9151 - val_dense_6_acc: 0.7838\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.94410 to 0.82927, saving model to model_bigrace_original_5.h5\n",
      "Epoch 18/100\n",
      "6362/6362 [==============================] - 4s 675us/step - loss: 0.9989 - dense_4_loss: 0.2422 - dense_6_loss: 0.7567 - dense_4_acc: 0.9024 - dense_6_acc: 0.7268 - val_loss: 0.8605 - val_dense_4_loss: 0.2536 - val_dense_6_loss: 0.6069 - val_dense_4_acc: 0.8875 - val_dense_6_acc: 0.7800\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/100\n",
      "6362/6362 [==============================] - 4s 659us/step - loss: 0.9654 - dense_4_loss: 0.2316 - dense_6_loss: 0.7338 - dense_4_acc: 0.9085 - dense_6_acc: 0.7359 - val_loss: 1.1355 - val_dense_4_loss: 0.3302 - val_dense_6_loss: 0.8053 - val_dense_4_acc: 0.8190 - val_dense_6_acc: 0.7190\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/100\n",
      "6362/6362 [==============================] - 4s 678us/step - loss: 0.9426 - dense_4_loss: 0.2299 - dense_6_loss: 0.7127 - dense_4_acc: 0.9084 - dense_6_acc: 0.7468 - val_loss: 0.8053 - val_dense_4_loss: 0.2428 - val_dense_6_loss: 0.5625 - val_dense_4_acc: 0.8950 - val_dense_6_acc: 0.7945\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.82927 to 0.80530, saving model to model_bigrace_original_5.h5\n",
      "Epoch 21/100\n",
      "6362/6362 [==============================] - 4s 661us/step - loss: 0.9246 - dense_4_loss: 0.2260 - dense_6_loss: 0.6986 - dense_4_acc: 0.9091 - dense_6_acc: 0.7458 - val_loss: 0.7648 - val_dense_4_loss: 0.2298 - val_dense_6_loss: 0.5350 - val_dense_4_acc: 0.8963 - val_dense_6_acc: 0.8133\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.80530 to 0.76481, saving model to model_bigrace_original_5.h5\n",
      "Epoch 22/100\n",
      "6362/6362 [==============================] - 4s 661us/step - loss: 0.8959 - dense_4_loss: 0.2239 - dense_6_loss: 0.6720 - dense_4_acc: 0.9101 - dense_6_acc: 0.7526 - val_loss: 0.8094 - val_dense_4_loss: 0.2433 - val_dense_6_loss: 0.5660 - val_dense_4_acc: 0.8925 - val_dense_6_acc: 0.7907\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6362/6362 [==============================] - 4s 671us/step - loss: 0.8787 - dense_4_loss: 0.2163 - dense_6_loss: 0.6624 - dense_4_acc: 0.9109 - dense_6_acc: 0.7619 - val_loss: 0.6745 - val_dense_4_loss: 0.2057 - val_dense_6_loss: 0.4688 - val_dense_4_acc: 0.9139 - val_dense_6_acc: 0.8353\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.76481 to 0.67446, saving model to model_bigrace_original_5.h5\n",
      "Epoch 24/100\n",
      "6362/6362 [==============================] - 4s 652us/step - loss: 0.8760 - dense_4_loss: 0.2212 - dense_6_loss: 0.6548 - dense_4_acc: 0.9131 - dense_6_acc: 0.7697 - val_loss: 0.6700 - val_dense_4_loss: 0.2115 - val_dense_6_loss: 0.4585 - val_dense_4_acc: 0.9045 - val_dense_6_acc: 0.8460\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.67446 to 0.66995, saving model to model_bigrace_original_5.h5\n",
      "Epoch 25/100\n",
      "6362/6362 [==============================] - 4s 649us/step - loss: 0.8527 - dense_4_loss: 0.2102 - dense_6_loss: 0.6425 - dense_4_acc: 0.9156 - dense_6_acc: 0.7768 - val_loss: 0.6314 - val_dense_4_loss: 0.1991 - val_dense_6_loss: 0.4323 - val_dense_4_acc: 0.9151 - val_dense_6_acc: 0.8624\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.66995 to 0.63136, saving model to model_bigrace_original_5.h5\n",
      "Epoch 26/100\n",
      "6362/6362 [==============================] - 4s 664us/step - loss: 0.8531 - dense_4_loss: 0.2202 - dense_6_loss: 0.6329 - dense_4_acc: 0.9074 - dense_6_acc: 0.7746 - val_loss: 0.6450 - val_dense_4_loss: 0.2015 - val_dense_6_loss: 0.4435 - val_dense_4_acc: 0.9239 - val_dense_6_acc: 0.8586\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/100\n",
      "6362/6362 [==============================] - 4s 659us/step - loss: 0.8329 - dense_4_loss: 0.2092 - dense_6_loss: 0.6237 - dense_4_acc: 0.9145 - dense_6_acc: 0.7818 - val_loss: 1.2660 - val_dense_4_loss: 0.3200 - val_dense_6_loss: 0.9461 - val_dense_4_acc: 0.8617 - val_dense_6_acc: 0.6807\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/100\n",
      "6362/6362 [==============================] - 4s 674us/step - loss: 0.8302 - dense_4_loss: 0.2064 - dense_6_loss: 0.6238 - dense_4_acc: 0.9176 - dense_6_acc: 0.7867 - val_loss: 0.7119 - val_dense_4_loss: 0.2219 - val_dense_6_loss: 0.4900 - val_dense_4_acc: 0.9076 - val_dense_6_acc: 0.8234\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/100\n",
      "6362/6362 [==============================] - 4s 651us/step - loss: 0.8044 - dense_4_loss: 0.2083 - dense_6_loss: 0.5961 - dense_4_acc: 0.9167 - dense_6_acc: 0.7914 - val_loss: 0.7202 - val_dense_4_loss: 0.2293 - val_dense_6_loss: 0.4909 - val_dense_4_acc: 0.8969 - val_dense_6_acc: 0.8290\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/100\n",
      "6362/6362 [==============================] - 4s 658us/step - loss: 0.7758 - dense_4_loss: 0.2036 - dense_6_loss: 0.5722 - dense_4_acc: 0.9180 - dense_6_acc: 0.8023 - val_loss: 0.7761 - val_dense_4_loss: 0.2564 - val_dense_6_loss: 0.5197 - val_dense_4_acc: 0.8705 - val_dense_6_acc: 0.8190\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/100\n",
      "6362/6362 [==============================] - 4s 694us/step - loss: 0.7711 - dense_4_loss: 0.2022 - dense_6_loss: 0.5689 - dense_4_acc: 0.9225 - dense_6_acc: 0.8034 - val_loss: 0.5750 - val_dense_4_loss: 0.1901 - val_dense_6_loss: 0.3849 - val_dense_4_acc: 0.9252 - val_dense_6_acc: 0.8781\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.63136 to 0.57496, saving model to model_bigrace_original_5.h5\n",
      "Epoch 32/100\n",
      "6362/6362 [==============================] - 5s 709us/step - loss: 0.7599 - dense_4_loss: 0.1988 - dense_6_loss: 0.5611 - dense_4_acc: 0.9197 - dense_6_acc: 0.8086 - val_loss: 0.8832 - val_dense_4_loss: 0.2450 - val_dense_6_loss: 0.6381 - val_dense_4_acc: 0.9013 - val_dense_6_acc: 0.7662\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/100\n",
      "6362/6362 [==============================] - 4s 656us/step - loss: 0.7507 - dense_4_loss: 0.1985 - dense_6_loss: 0.5522 - dense_4_acc: 0.9217 - dense_6_acc: 0.8115 - val_loss: 0.6516 - val_dense_4_loss: 0.2047 - val_dense_6_loss: 0.4469 - val_dense_4_acc: 0.9151 - val_dense_6_acc: 0.8523\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/100\n",
      "6362/6362 [==============================] - 4s 654us/step - loss: 0.7706 - dense_4_loss: 0.2040 - dense_6_loss: 0.5666 - dense_4_acc: 0.9184 - dense_6_acc: 0.8029 - val_loss: 0.6677 - val_dense_4_loss: 0.2034 - val_dense_6_loss: 0.4643 - val_dense_4_acc: 0.9258 - val_dense_6_acc: 0.8410\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/100\n",
      "6362/6362 [==============================] - 4s 639us/step - loss: 0.7511 - dense_4_loss: 0.2044 - dense_6_loss: 0.5467 - dense_4_acc: 0.9173 - dense_6_acc: 0.8103 - val_loss: 0.9498 - val_dense_4_loss: 0.2753 - val_dense_6_loss: 0.6744 - val_dense_4_acc: 0.8843 - val_dense_6_acc: 0.7693\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/100\n",
      "6362/6362 [==============================] - 4s 653us/step - loss: 0.7374 - dense_4_loss: 0.1957 - dense_6_loss: 0.5417 - dense_4_acc: 0.9205 - dense_6_acc: 0.8130 - val_loss: 0.5544 - val_dense_4_loss: 0.1895 - val_dense_6_loss: 0.3648 - val_dense_4_acc: 0.9145 - val_dense_6_acc: 0.8831\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.57496 to 0.55437, saving model to model_bigrace_original_5.h5\n",
      "Epoch 37/100\n",
      "6362/6362 [==============================] - 4s 689us/step - loss: 0.7315 - dense_4_loss: 0.1973 - dense_6_loss: 0.5342 - dense_4_acc: 0.9187 - dense_6_acc: 0.8214 - val_loss: 0.5532 - val_dense_4_loss: 0.1853 - val_dense_6_loss: 0.3679 - val_dense_4_acc: 0.9239 - val_dense_6_acc: 0.8856\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.55437 to 0.55324, saving model to model_bigrace_original_5.h5\n",
      "Epoch 38/100\n",
      "6362/6362 [==============================] - 4s 679us/step - loss: 0.7138 - dense_4_loss: 0.1952 - dense_6_loss: 0.5186 - dense_4_acc: 0.9227 - dense_6_acc: 0.8216 - val_loss: 0.8177 - val_dense_4_loss: 0.2476 - val_dense_6_loss: 0.5701 - val_dense_4_acc: 0.9063 - val_dense_6_acc: 0.8089\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/100\n",
      "6362/6362 [==============================] - 4s 658us/step - loss: 0.7139 - dense_4_loss: 0.1981 - dense_6_loss: 0.5158 - dense_4_acc: 0.9205 - dense_6_acc: 0.8221 - val_loss: 0.5589 - val_dense_4_loss: 0.1906 - val_dense_6_loss: 0.3683 - val_dense_4_acc: 0.9183 - val_dense_6_acc: 0.8831\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/100\n",
      "6362/6362 [==============================] - 4s 677us/step - loss: 0.7088 - dense_4_loss: 0.1953 - dense_6_loss: 0.5136 - dense_4_acc: 0.9236 - dense_6_acc: 0.8263 - val_loss: 0.6417 - val_dense_4_loss: 0.2006 - val_dense_6_loss: 0.4411 - val_dense_4_acc: 0.9271 - val_dense_6_acc: 0.8473\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/100\n",
      "6362/6362 [==============================] - 4s 695us/step - loss: 0.6930 - dense_4_loss: 0.1915 - dense_6_loss: 0.5015 - dense_4_acc: 0.9266 - dense_6_acc: 0.8276 - val_loss: 0.5611 - val_dense_4_loss: 0.1928 - val_dense_6_loss: 0.3683 - val_dense_4_acc: 0.9164 - val_dense_6_acc: 0.8856\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/100\n",
      "6362/6362 [==============================] - 4s 678us/step - loss: 0.6855 - dense_4_loss: 0.1856 - dense_6_loss: 0.4999 - dense_4_acc: 0.9268 - dense_6_acc: 0.8331 - val_loss: 0.5722 - val_dense_4_loss: 0.1957 - val_dense_6_loss: 0.3765 - val_dense_4_acc: 0.9177 - val_dense_6_acc: 0.8699\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/100\n",
      "6362/6362 [==============================] - 4s 648us/step - loss: 0.6739 - dense_4_loss: 0.1855 - dense_6_loss: 0.4885 - dense_4_acc: 0.9275 - dense_6_acc: 0.8323 - val_loss: 0.6922 - val_dense_4_loss: 0.2057 - val_dense_6_loss: 0.4865 - val_dense_4_acc: 0.9239 - val_dense_6_acc: 0.8322\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/100\n",
      "6362/6362 [==============================] - 4s 667us/step - loss: 0.6812 - dense_4_loss: 0.1874 - dense_6_loss: 0.4937 - dense_4_acc: 0.9275 - dense_6_acc: 0.8329 - val_loss: 0.5477 - val_dense_4_loss: 0.1922 - val_dense_6_loss: 0.3555 - val_dense_4_acc: 0.9183 - val_dense_6_acc: 0.8825\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.55324 to 0.54766, saving model to model_bigrace_original_5.h5\n",
      "Epoch 45/100\n",
      "6362/6362 [==============================] - 4s 673us/step - loss: 0.6814 - dense_4_loss: 0.1918 - dense_6_loss: 0.4895 - dense_4_acc: 0.9228 - dense_6_acc: 0.8329 - val_loss: 0.8008 - val_dense_4_loss: 0.2367 - val_dense_6_loss: 0.5641 - val_dense_4_acc: 0.9089 - val_dense_6_acc: 0.7995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/100\n",
      "6362/6362 [==============================] - 4s 660us/step - loss: 0.6667 - dense_4_loss: 0.1863 - dense_6_loss: 0.4804 - dense_4_acc: 0.9253 - dense_6_acc: 0.8433 - val_loss: 0.5625 - val_dense_4_loss: 0.1899 - val_dense_6_loss: 0.3725 - val_dense_4_acc: 0.9195 - val_dense_6_acc: 0.8793\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/100\n",
      "6362/6362 [==============================] - 4s 664us/step - loss: 0.6685 - dense_4_loss: 0.1835 - dense_6_loss: 0.4850 - dense_4_acc: 0.9249 - dense_6_acc: 0.8354 - val_loss: 0.4926 - val_dense_4_loss: 0.1719 - val_dense_6_loss: 0.3207 - val_dense_4_acc: 0.9315 - val_dense_6_acc: 0.8988\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.54766 to 0.49257, saving model to model_bigrace_original_5.h5\n",
      "Epoch 48/100\n",
      "6362/6362 [==============================] - 4s 645us/step - loss: 0.6659 - dense_4_loss: 0.1825 - dense_6_loss: 0.4835 - dense_4_acc: 0.9304 - dense_6_acc: 0.8354 - val_loss: 0.5043 - val_dense_4_loss: 0.1719 - val_dense_6_loss: 0.3323 - val_dense_4_acc: 0.9378 - val_dense_6_acc: 0.8957\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/100\n",
      "6362/6362 [==============================] - 4s 663us/step - loss: 0.6561 - dense_4_loss: 0.1844 - dense_6_loss: 0.4717 - dense_4_acc: 0.9261 - dense_6_acc: 0.8464 - val_loss: 0.5032 - val_dense_4_loss: 0.1808 - val_dense_6_loss: 0.3224 - val_dense_4_acc: 0.9221 - val_dense_6_acc: 0.8988\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/100\n",
      "6362/6362 [==============================] - 4s 687us/step - loss: 0.6407 - dense_4_loss: 0.1854 - dense_6_loss: 0.4553 - dense_4_acc: 0.9247 - dense_6_acc: 0.8464 - val_loss: 0.5473 - val_dense_4_loss: 0.1923 - val_dense_6_loss: 0.3550 - val_dense_4_acc: 0.9346 - val_dense_6_acc: 0.8812\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/100\n",
      "6362/6362 [==============================] - 4s 655us/step - loss: 0.6359 - dense_4_loss: 0.1758 - dense_6_loss: 0.4601 - dense_4_acc: 0.9308 - dense_6_acc: 0.8425 - val_loss: 0.5110 - val_dense_4_loss: 0.1793 - val_dense_6_loss: 0.3317 - val_dense_4_acc: 0.9283 - val_dense_6_acc: 0.8931\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/100\n",
      "6362/6362 [==============================] - 4s 693us/step - loss: 0.6353 - dense_4_loss: 0.1826 - dense_6_loss: 0.4527 - dense_4_acc: 0.9263 - dense_6_acc: 0.8518 - val_loss: 0.5786 - val_dense_4_loss: 0.1816 - val_dense_6_loss: 0.3969 - val_dense_4_acc: 0.9283 - val_dense_6_acc: 0.8768\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/100\n",
      "6362/6362 [==============================] - 4s 645us/step - loss: 0.6293 - dense_4_loss: 0.1861 - dense_6_loss: 0.4432 - dense_4_acc: 0.9263 - dense_6_acc: 0.8519 - val_loss: 0.4683 - val_dense_4_loss: 0.1675 - val_dense_6_loss: 0.3008 - val_dense_4_acc: 0.9309 - val_dense_6_acc: 0.9076\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.49257 to 0.46831, saving model to model_bigrace_original_5.h5\n",
      "Epoch 54/100\n",
      "6362/6362 [==============================] - 4s 654us/step - loss: 0.6218 - dense_4_loss: 0.1715 - dense_6_loss: 0.4503 - dense_4_acc: 0.9316 - dense_6_acc: 0.8500 - val_loss: 0.7435 - val_dense_4_loss: 0.2301 - val_dense_6_loss: 0.5134 - val_dense_4_acc: 0.8957 - val_dense_6_acc: 0.8253\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/100\n",
      "6362/6362 [==============================] - 4s 683us/step - loss: 0.5994 - dense_4_loss: 0.1750 - dense_6_loss: 0.4244 - dense_4_acc: 0.9329 - dense_6_acc: 0.8614 - val_loss: 0.5549 - val_dense_4_loss: 0.1967 - val_dense_6_loss: 0.3582 - val_dense_4_acc: 0.9133 - val_dense_6_acc: 0.8869\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/100\n",
      "6362/6362 [==============================] - 4s 649us/step - loss: 0.6096 - dense_4_loss: 0.1761 - dense_6_loss: 0.4335 - dense_4_acc: 0.9288 - dense_6_acc: 0.8533 - val_loss: 0.5072 - val_dense_4_loss: 0.1817 - val_dense_6_loss: 0.3255 - val_dense_4_acc: 0.9265 - val_dense_6_acc: 0.8957\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/100\n",
      "6362/6362 [==============================] - 4s 666us/step - loss: 0.6246 - dense_4_loss: 0.1791 - dense_6_loss: 0.4455 - dense_4_acc: 0.9297 - dense_6_acc: 0.8532 - val_loss: 0.5684 - val_dense_4_loss: 0.1966 - val_dense_6_loss: 0.3718 - val_dense_4_acc: 0.9309 - val_dense_6_acc: 0.8787\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/100\n",
      "6362/6362 [==============================] - 4s 648us/step - loss: 0.6053 - dense_4_loss: 0.1752 - dense_6_loss: 0.4301 - dense_4_acc: 0.9343 - dense_6_acc: 0.8596 - val_loss: 0.5376 - val_dense_4_loss: 0.1919 - val_dense_6_loss: 0.3457 - val_dense_4_acc: 0.9290 - val_dense_6_acc: 0.8843\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/100\n",
      "6362/6362 [==============================] - 4s 676us/step - loss: 0.5909 - dense_4_loss: 0.1672 - dense_6_loss: 0.4237 - dense_4_acc: 0.9362 - dense_6_acc: 0.8604 - val_loss: 0.4912 - val_dense_4_loss: 0.1746 - val_dense_6_loss: 0.3166 - val_dense_4_acc: 0.9415 - val_dense_6_acc: 0.9007\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/100\n",
      "6362/6362 [==============================] - 4s 697us/step - loss: 0.5917 - dense_4_loss: 0.1671 - dense_6_loss: 0.4246 - dense_4_acc: 0.9340 - dense_6_acc: 0.8560 - val_loss: 0.4787 - val_dense_4_loss: 0.1721 - val_dense_6_loss: 0.3065 - val_dense_4_acc: 0.9378 - val_dense_6_acc: 0.9120\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/100\n",
      "6362/6362 [==============================] - 4s 652us/step - loss: 0.6061 - dense_4_loss: 0.1709 - dense_6_loss: 0.4352 - dense_4_acc: 0.9338 - dense_6_acc: 0.8615 - val_loss: 0.5774 - val_dense_4_loss: 0.1956 - val_dense_6_loss: 0.3819 - val_dense_4_acc: 0.9195 - val_dense_6_acc: 0.8755\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/100\n",
      "6362/6362 [==============================] - 4s 662us/step - loss: 0.5950 - dense_4_loss: 0.1737 - dense_6_loss: 0.4212 - dense_4_acc: 0.9326 - dense_6_acc: 0.8548 - val_loss: 0.4446 - val_dense_4_loss: 0.1657 - val_dense_6_loss: 0.2789 - val_dense_4_acc: 0.9346 - val_dense_6_acc: 0.9189\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.46831 to 0.44460, saving model to model_bigrace_original_5.h5\n",
      "Epoch 63/100\n",
      "6362/6362 [==============================] - 4s 683us/step - loss: 0.5936 - dense_4_loss: 0.1780 - dense_6_loss: 0.4157 - dense_4_acc: 0.9294 - dense_6_acc: 0.8577 - val_loss: 0.4605 - val_dense_4_loss: 0.1675 - val_dense_6_loss: 0.2930 - val_dense_4_acc: 0.9359 - val_dense_6_acc: 0.9082\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/100\n",
      "6362/6362 [==============================] - 4s 702us/step - loss: 0.5643 - dense_4_loss: 0.1720 - dense_6_loss: 0.3923 - dense_4_acc: 0.9351 - dense_6_acc: 0.8680 - val_loss: 0.9937 - val_dense_4_loss: 0.3583 - val_dense_6_loss: 0.6354 - val_dense_4_acc: 0.8705 - val_dense_6_acc: 0.7838\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/100\n",
      "6362/6362 [==============================] - 5s 711us/step - loss: 0.5905 - dense_4_loss: 0.1753 - dense_6_loss: 0.4152 - dense_4_acc: 0.9308 - dense_6_acc: 0.8637 - val_loss: 0.4741 - val_dense_4_loss: 0.1793 - val_dense_6_loss: 0.2948 - val_dense_4_acc: 0.9340 - val_dense_6_acc: 0.9101\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/100\n",
      "6362/6362 [==============================] - 4s 704us/step - loss: 0.5631 - dense_4_loss: 0.1583 - dense_6_loss: 0.4047 - dense_4_acc: 0.9379 - dense_6_acc: 0.8629 - val_loss: 0.4567 - val_dense_4_loss: 0.1650 - val_dense_6_loss: 0.2917 - val_dense_4_acc: 0.9365 - val_dense_6_acc: 0.9089\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/100\n",
      "6362/6362 [==============================] - 4s 649us/step - loss: 0.5795 - dense_4_loss: 0.1754 - dense_6_loss: 0.4040 - dense_4_acc: 0.9338 - dense_6_acc: 0.8659 - val_loss: 0.4856 - val_dense_4_loss: 0.1748 - val_dense_6_loss: 0.3108 - val_dense_4_acc: 0.9283 - val_dense_6_acc: 0.9007\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/100\n",
      "6362/6362 [==============================] - 4s 683us/step - loss: 0.5686 - dense_4_loss: 0.1665 - dense_6_loss: 0.4021 - dense_4_acc: 0.9376 - dense_6_acc: 0.8700 - val_loss: 0.4469 - val_dense_4_loss: 0.1687 - val_dense_6_loss: 0.2782 - val_dense_4_acc: 0.9365 - val_dense_6_acc: 0.9164\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6362/6362 [==============================] - 4s 669us/step - loss: 0.5511 - dense_4_loss: 0.1606 - dense_6_loss: 0.3905 - dense_4_acc: 0.9351 - dense_6_acc: 0.8713 - val_loss: 0.5857 - val_dense_4_loss: 0.2007 - val_dense_6_loss: 0.3850 - val_dense_4_acc: 0.9170 - val_dense_6_acc: 0.8781\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/100\n",
      "6362/6362 [==============================] - 4s 649us/step - loss: 0.5735 - dense_4_loss: 0.1722 - dense_6_loss: 0.4014 - dense_4_acc: 0.9340 - dense_6_acc: 0.8631 - val_loss: 0.4465 - val_dense_4_loss: 0.1659 - val_dense_6_loss: 0.2806 - val_dense_4_acc: 0.9390 - val_dense_6_acc: 0.9170\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/100\n",
      "6362/6362 [==============================] - 4s 654us/step - loss: 0.5794 - dense_4_loss: 0.1696 - dense_6_loss: 0.4099 - dense_4_acc: 0.9315 - dense_6_acc: 0.8662 - val_loss: 0.5097 - val_dense_4_loss: 0.1774 - val_dense_6_loss: 0.3324 - val_dense_4_acc: 0.9277 - val_dense_6_acc: 0.8875\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/100\n",
      "6362/6362 [==============================] - 4s 651us/step - loss: 0.5468 - dense_4_loss: 0.1575 - dense_6_loss: 0.3893 - dense_4_acc: 0.9392 - dense_6_acc: 0.8681 - val_loss: 0.4809 - val_dense_4_loss: 0.1768 - val_dense_6_loss: 0.3040 - val_dense_4_acc: 0.9315 - val_dense_6_acc: 0.9026\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/100\n",
      "6362/6362 [==============================] - 4s 643us/step - loss: 0.5754 - dense_4_loss: 0.1663 - dense_6_loss: 0.4092 - dense_4_acc: 0.9354 - dense_6_acc: 0.8684 - val_loss: 0.4546 - val_dense_4_loss: 0.1680 - val_dense_6_loss: 0.2865 - val_dense_4_acc: 0.9346 - val_dense_6_acc: 0.9164\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/100\n",
      "6362/6362 [==============================] - 4s 643us/step - loss: 0.5594 - dense_4_loss: 0.1654 - dense_6_loss: 0.3940 - dense_4_acc: 0.9356 - dense_6_acc: 0.8713 - val_loss: 0.6405 - val_dense_4_loss: 0.2148 - val_dense_6_loss: 0.4256 - val_dense_4_acc: 0.9227 - val_dense_6_acc: 0.8605\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/100\n",
      "6362/6362 [==============================] - 4s 686us/step - loss: 0.5625 - dense_4_loss: 0.1628 - dense_6_loss: 0.3997 - dense_4_acc: 0.9338 - dense_6_acc: 0.8725 - val_loss: 0.4798 - val_dense_4_loss: 0.1819 - val_dense_6_loss: 0.2979 - val_dense_4_acc: 0.9214 - val_dense_6_acc: 0.9057\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/100\n",
      "6362/6362 [==============================] - 4s 654us/step - loss: 0.5537 - dense_4_loss: 0.1704 - dense_6_loss: 0.3832 - dense_4_acc: 0.9321 - dense_6_acc: 0.8744 - val_loss: 0.8358 - val_dense_4_loss: 0.2874 - val_dense_6_loss: 0.5484 - val_dense_4_acc: 0.8957 - val_dense_6_acc: 0.8033\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 77/100\n",
      "6362/6362 [==============================] - 4s 675us/step - loss: 0.5104 - dense_4_loss: 0.1549 - dense_6_loss: 0.3555 - dense_4_acc: 0.9417 - dense_6_acc: 0.8823 - val_loss: 0.5520 - val_dense_4_loss: 0.1980 - val_dense_6_loss: 0.3540 - val_dense_4_acc: 0.9258 - val_dense_6_acc: 0.8862\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/100\n",
      "6362/6362 [==============================] - 4s 664us/step - loss: 0.5292 - dense_4_loss: 0.1647 - dense_6_loss: 0.3645 - dense_4_acc: 0.9345 - dense_6_acc: 0.8837 - val_loss: 0.5051 - val_dense_4_loss: 0.1787 - val_dense_6_loss: 0.3264 - val_dense_4_acc: 0.9296 - val_dense_6_acc: 0.9001\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 79/100\n",
      "6362/6362 [==============================] - 4s 680us/step - loss: 0.5524 - dense_4_loss: 0.1619 - dense_6_loss: 0.3905 - dense_4_acc: 0.9387 - dense_6_acc: 0.8730 - val_loss: 0.5040 - val_dense_4_loss: 0.1732 - val_dense_6_loss: 0.3308 - val_dense_4_acc: 0.9327 - val_dense_6_acc: 0.9007\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 80/100\n",
      "6362/6362 [==============================] - 4s 680us/step - loss: 0.5281 - dense_4_loss: 0.1592 - dense_6_loss: 0.3689 - dense_4_acc: 0.9363 - dense_6_acc: 0.8735 - val_loss: 0.4611 - val_dense_4_loss: 0.1698 - val_dense_6_loss: 0.2912 - val_dense_4_acc: 0.9384 - val_dense_6_acc: 0.9082\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/100\n",
      "6362/6362 [==============================] - 4s 667us/step - loss: 0.5384 - dense_4_loss: 0.1650 - dense_6_loss: 0.3734 - dense_4_acc: 0.9359 - dense_6_acc: 0.8790 - val_loss: 0.4407 - val_dense_4_loss: 0.1674 - val_dense_6_loss: 0.2733 - val_dense_4_acc: 0.9321 - val_dense_6_acc: 0.9164\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.44460 to 0.44067, saving model to model_bigrace_original_5.h5\n",
      "Epoch 82/100\n",
      "6362/6362 [==============================] - 4s 675us/step - loss: 0.5241 - dense_4_loss: 0.1571 - dense_6_loss: 0.3670 - dense_4_acc: 0.9381 - dense_6_acc: 0.8821 - val_loss: 0.8500 - val_dense_4_loss: 0.2556 - val_dense_6_loss: 0.5943 - val_dense_4_acc: 0.8906 - val_dense_6_acc: 0.7832\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/100\n",
      "6362/6362 [==============================] - 4s 643us/step - loss: 0.5259 - dense_4_loss: 0.1539 - dense_6_loss: 0.3720 - dense_4_acc: 0.9411 - dense_6_acc: 0.8816 - val_loss: 0.7803 - val_dense_4_loss: 0.2621 - val_dense_6_loss: 0.5182 - val_dense_4_acc: 0.9038 - val_dense_6_acc: 0.8196\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 84/100\n",
      "6362/6362 [==============================] - 4s 679us/step - loss: 0.5587 - dense_4_loss: 0.1661 - dense_6_loss: 0.3926 - dense_4_acc: 0.9360 - dense_6_acc: 0.8760 - val_loss: 0.4366 - val_dense_4_loss: 0.1686 - val_dense_6_loss: 0.2681 - val_dense_4_acc: 0.9359 - val_dense_6_acc: 0.9164\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.44067 to 0.43663, saving model to model_bigrace_original_5.h5\n",
      "Epoch 85/100\n",
      "6362/6362 [==============================] - 4s 668us/step - loss: 0.5250 - dense_4_loss: 0.1592 - dense_6_loss: 0.3658 - dense_4_acc: 0.9374 - dense_6_acc: 0.8780 - val_loss: 0.5077 - val_dense_4_loss: 0.1796 - val_dense_6_loss: 0.3281 - val_dense_4_acc: 0.9321 - val_dense_6_acc: 0.8938\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/100\n",
      "6362/6362 [==============================] - 4s 677us/step - loss: 0.5134 - dense_4_loss: 0.1547 - dense_6_loss: 0.3587 - dense_4_acc: 0.9406 - dense_6_acc: 0.8821 - val_loss: 0.4846 - val_dense_4_loss: 0.1667 - val_dense_6_loss: 0.3179 - val_dense_4_acc: 0.9327 - val_dense_6_acc: 0.8925\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/100\n",
      "6362/6362 [==============================] - 4s 663us/step - loss: 0.5224 - dense_4_loss: 0.1578 - dense_6_loss: 0.3646 - dense_4_acc: 0.9400 - dense_6_acc: 0.8824 - val_loss: 1.4654 - val_dense_4_loss: 0.5278 - val_dense_6_loss: 0.9376 - val_dense_4_acc: 0.8404 - val_dense_6_acc: 0.6750\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 88/100\n",
      " 704/6362 [==>...........................] - ETA: 3s - loss: 0.4785 - dense_4_loss: 0.1124 - dense_6_loss: 0.3660 - dense_4_acc: 0.9574 - dense_6_acc: 0.8764"
     ]
    }
   ],
   "source": [
    "h = model.fit(images, [labels_speed, labels_dir], batch_size=64, epochs=100, validation_split=0.2, verbose=1, callbacks=[best_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print History graph\n",
    "historydf = pd.DataFrame(h.history, index=h.epoch)\n",
    "historydf.plot(ylim=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
